# -*- coding: utf-8 -*-
"""CyclingPerformance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18iAg8mLKoiqdzHdwlRj00Hq1BdnfoyHh

# Cycling Performance Analysis

As a passionate cyclist with a focus on road biking, I am embarking on an analysis project to enhance performance and gain insights in this field. The project involves tracking and collecting data from road biking activities using advanced measurement equipment. This data includes variables such as moving time, distance, elevation gain, calories burned, average speed, heart rate, power output, and training load.

By analyzing this data, I aim to provide valuable information and recommendations to fellow road cyclists. This includes creating personalized dashboards and visualizations that offer descriptive summaries of their performance. These tools will enable riders to easily understand their current status and identify areas for improvement.

Moreover, the project will involve the development of predictive models. These models will utilize the collected variables to estimate performance in a certain time frame. Additionally, the data can be leveraged to create training plans that are tailored to individual riders' goals and objectives.

Ultimately, this analysis project aims to empower road cyclists by providing them with valuable insights, performance tracking tools, and personalized recommendations. By leveraging data-driven approaches, we can enhance the road biking experience and support individuals in reaching their maximum potential on the road.
"""

!pip install pandasai

# Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates #for time series plot
import seaborn as sns
sns.set(font_scale = 1.2, style = 'ticks')
import datetime as dt
import missingno as msno #Python library for the exploratory visualization of missing data
import plotly.express as px #library for plotly visualization

# pandas ai
from pandasai import PandasAI
from pandasai.llm.openai import OpenAI

# sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

"""# Import data and data wrangling"""

# prompt: mount drive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/DataScience/Dataset/cycling_perf_June.csv')

df.head()

# df.info()
df.isna().sum()

df.columns

# select the columns you want to use
# create a function for that
def select_columns(df, columns):
  return df[columns]

select = ['start_date_local', 'type', 'moving_time', 'distance',
       'elapsed_time', 'total_elevation_gain', 'max_speed', 'average_speed',
       'has_heartrate', 'max_heartrate', 'average_heartrate',
       'average_cadence', 'calories', 'device_watts', 'icu_average_watts',
       'icu_normalized_watts', 'icu_joules', 'icu_intensity',
       'icu_training_load', 'pace',
       'icu_fatigue', 'icu_fitness', 'icu_eftp',
       'power_load', 'hr_load', 'hr_z1_secs', 'hr_z2_secs', 'hr_z3_secs',
       'hr_z4_secs', 'hr_z5_secs', 'hr_z6_secs', 'hr_z7_secs', 'z1_secs',
       'z2_secs', 'z3_secs', 'z4_secs', 'z5_secs', 'z6_secs', 'z7_secs',
       'sweet_spot_secs', 'icu_weight', 'icu_pm_ftp', 'icu_pm_cp']

df_selected = select_columns(df, select)

# prompt: filter df_selected with type is 'ride' or 'VirtualRide'
df_filtered = df_selected[df_selected['type'].isin(['Ride', 'VirtualRide'])]
df_filtered.head()

df_filtered.head()
print(df_filtered.shape)

df_filtered.info()

# We are focusing on training ride only, so we would exclude commute rides and other irrelevant rides.
df_filtered = df_filtered[~df_filtered['distance'].isna()] # Since those are done on peleton without power
df_filtered = df_filtered[~df_filtered['icu_average_watts'].isna()] # Those are commute ride
df_filtered.shape

## Check individual values for missing values
# print(df_filtered.isna())
## Check each column for missing values
# print(df_filtered.isna().any())

# Bar plot of missing values by variable
# Plotting the bar plot
ax = df_filtered.isna().sum().plot(kind='bar')
# Adjusting the tick width
ax.tick_params(axis='x', width=1)
plt.xticks(fontsize=8)
# Adding the title
plt.title("Missing Values by Column", fontsize=15)
# Adding the y-axis label
plt.ylabel("Number of Missing Values", fontsize=15)
# Show plot
plt.show()

# Show missing values
msno.matrix(df_filtered)

df_filtered

# missing_values = df_filtered.isna().sum()
# missing_values = missing_values[missing_values > 0]
# print(missing_values)

# Wrap it into a function
def get_columns_with_missing_values(df):
    missing_values = df.isna().sum()
    columns_with_missing_values = missing_values[missing_values > 0]

    if len(columns_with_missing_values) > 0:
      print("Columns with missing values:")
      print(columns_with_missing_values)
    else:
      print("There are no missing values.")

# Example usage:
get_columns_with_missing_values(df_filtered)

# Convert "start_date_local" column to datetime
df_filtered['start_date_local'] = pd.to_datetime(df_filtered['start_date_local'])

# Extract relevant features from datetime column
df_filtered['year'] = df_filtered['start_date_local'].dt.year
df_filtered['month'] = df_filtered['start_date_local'].dt.month
df_filtered['day'] = df_filtered['start_date_local'].dt.day
df_filtered['hour'] = df_filtered['start_date_local'].dt.hour
df_filtered['minute'] = df_filtered['start_date_local'].dt.minute
df_filtered['day_of_week'] = df_filtered['start_date_local'].dt.dayofweek
df_filtered = df_filtered.drop('start_date_local', axis = 1)

# Encode categorical variable 'type' using one-hot encoding
cat_var = ['type','has_heartrate','device_watts']
df_filtered_encoded = pd.get_dummies(df_filtered, columns = cat_var, drop_first=True)

# Splitting the dataset into missing and non-missing data
df_missing = df_filtered_encoded[df_filtered_encoded.isna().any(axis=1)]
df_non_missing = df_filtered_encoded.dropna()

df_filtered_encoded.info()

missing_list = ['max_heartrate', 'average_heartrate', 'calories', 'icu_eftp', 'hr_load',
                'hr_z1_secs', 'hr_z2_secs', 'hr_z3_secs', 'hr_z4_secs', 'hr_z5_secs',
                'hr_z6_secs', 'hr_z7_secs']
# Iterate over each feature with missing values
for feature in missing_list:
    # Splitting non-missing data into predictors and target variable
    X = df_non_missing.drop(missing_list, axis=1)
    y = df_non_missing[feature]

    # Splitting into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Building a linear regression model
    regressor = LinearRegression()
    regressor.fit(X_train, y_train)

    # Predicting missing values
    X_missing = df_missing.drop(missing_list, axis=1)
    predicted_values = regressor.predict(X_missing)

    # Replacing missing values in df_filtered_encoded with predicted values
    df_filtered_encoded.loc[df_missing.index, feature] = predicted_values

df_final = df_filtered_encoded

get_columns_with_missing_values(df_final)

# Show missing values
msno.matrix(df_final)

df_final['date'] = df['start_date']
df_final
df_final.to_csv('/content/drive/MyDrive/DataScience/Dataset/cycling_perf_June_cleaned.csv')

"""# EDA (Exploratory Data Analysis)"""

df_final.head()

df_final.info()

# Convert the "date" column to datetime format
df_final['date'] = pd.to_datetime(df_final['date']).dt.tz_localize(None)

# Group by week and aggregate the values
weekly_aggregation = df_final.groupby(pd.Grouper(key='date', freq='W-SUN')).agg({
    'moving_time': 'sum',
    'distance': 'sum',
    'total_elevation_gain': 'sum',
    'icu_training_load': 'sum'
}).reset_index()

# Rename the column for the week
weekly_aggregation.rename(columns={'date': 'week'}, inplace=True)

# Convert the unit to "km"
weekly_aggregation["distance"] = weekly_aggregation["distance"] / 1000
weekly_aggregation["moving_time"] = weekly_aggregation["moving_time"] / 3600 # to Hrs

weekly_aggregation

# Show the training hours status over time
fig, ax = plt.subplots()
ax.plot(weekly_aggregation['week'], weekly_aggregation['moving_time'])
ax.set(xlabel='week', ylabel='training_time')
# Set the x-axis limits
ax.set_xlim([weekly_aggregation['week'].min() - pd.Timedelta(days=7), weekly_aggregation['week'].max() + pd.Timedelta(days=7)])


# Use plotly to have interactive plot
fig = px.line(weekly_aggregation, x='week', y='moving_time')
fig.show()

# prompt: create monthly aggregation for power(average), sum of distance and time and training load

monthly_aggregation = df_final.groupby(pd.Grouper(key='date', freq='M')).agg({
    'icu_normalized_watts': 'mean',
    'distance': 'sum',
    'moving_time': 'sum',
    'icu_training_load': 'sum'
}).reset_index()

monthly_aggregation['month'] = monthly_aggregation['date']
monthly_aggregation

# prompt: create visualization for icu_normalized_watts and icu_training_load over time, make it into 2 different plot

# Plot for icu_normalized_watts
fig, ax = plt.subplots()
ax.plot(monthly_aggregation['month'], monthly_aggregation['icu_normalized_watts'])
ax.set(xlabel='month', ylabel='icu_normalized_watts')


# Plot for icu_training_load
fig, ax = plt.subplots()
ax.plot(monthly_aggregation['month'], monthly_aggregation['icu_training_load'])
ax.set(xlabel='month', ylabel='icu_training_load')

"""# Modeling
## Prophet model
### I. For fitness level
"""

# Function for extracting data for prophet
def preprocess_data(dataframe, date_column, y_column):
    prophet_data = pd.DataFrame()
    prophet_data['y'] = dataframe[y_column]
    prophet_data['ds'] = dataframe[date_column]
    prophet_data['ds'] = pd.to_datetime(prophet_data['ds']).dt.tz_localize(None)
    prophet_data['ds'] = prophet_data['ds'].dt.date
    prophet_data['ds'] = pd.to_datetime(prophet_data['ds'])
    return prophet_data

# Replace 'your_data.csv' with your actual data source
processed_data = preprocess_data(df_final, "date", "icu_fitness")
print(processed_data)

# Prophet model
import prophet
from prophet import Prophet
from prophet.plot import add_changepoints_to_plot

# Daily data
prophetdata = pd.DataFrame()
prophetdata['y'] = df_final["icu_fitness"]
prophetdata['ds'] = df_final["date"]
prophetdata['ds'] = pd.to_datetime(prophetdata['ds'])
prophetdata['ds'] = prophetdata['ds'].dt.tz_localize(None)
prophetdata['ds'] = prophetdata['ds'].dt.date
prophetdata['ds'] = pd.to_datetime(prophetdata['ds'])
print(prophetdata)

# aggregate by week
df_weekly = prophetdata.groupby(pd.Grouper(key='ds', freq='W')).mean().reset_index()
df_weekly

# create prophet model with daily based data
  m = Prophet( yearly_seasonality=True,weekly_seasonality = True)
  #m.add_country_holidays(country_name='US')
  m.fit(prophetdata)
  future = m.make_future_dataframe(periods=365, freq='D') # make future DF with 365 days
  future.tail()
  forecast = m.predict(future)
  fig1 = m.plot(forecast)
  fig2 = m.plot_components(forecast)
  a = add_changepoints_to_plot(fig1.gca(), m, forecast)

df = prophetdata
# Python
import itertools
import numpy as np
import pandas as pd
from prophet.diagnostics import cross_validation
from prophet.diagnostics import performance_metrics

param_grid = {
    'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5],
    'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0],
}

# Generate all combinations of parameters
all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]
rmses = []  # Store the RMSEs for each params here

# Use cross validation to evaluate all parameters
for params in all_params:
    m = Prophet(**params).fit(df)  # Fit model with given params
    df_cv = cross_validation(m, horizon='30 days', parallel="processes")
    df_p = performance_metrics(df_cv, rolling_window=1)
    rmses.append(df_p['rmse'].values[0])

# Find the best parameters
tuning_results = pd.DataFrame(all_params)
tuning_results['rmse'] = rmses
print(tuning_results)

from prophet.plot import plot_plotly, plot_components_plotly
plot_plotly(m, forecast)

plot_components_plotly(m, forecast)

# Let's check what's in the model.predict()
forecast

## Wrap into function

def prophet_model(prophetdata):
  """
  This function is used to train a prophet model on a dataframe.
  and plot the forecast.  , weekly_seasonality= True
  """
  m = Prophet(changepoint_prior_scale=0.1, yearly_seasonality=True,weekly_seasonality = True)
  #m.add_country_holidays(country_name='US')
  m.fit(prophetdata)
  future = m.make_future_dataframe(periods=52, freq='W')
  future.tail()
  forecast = m.predict(future)
  fig1 = m.plot(forecast)
  fig2 = m.plot_components(forecast)
  a = add_changepoints_to_plot(fig1.gca(), m, forecast)

prophet_model(prophetdata)

# On a weekly basis
prophet_model(df_weekly)

"""## For Power level

- Saturating Forecasts:

By default, Prophet uses a linear model for its forecast. When forecasting growth, there is usually some maximum achievable point: total market size, total population size, etc. This is called the carrying capacity, and the forecast should saturate at this point. Prophet allows you to make forecasts using a logistic growth trend model, with a specified carrying capacity.
"""

# visualize the power dist for outliers
plt.figure(figsize=(10, 6))
plt.hist(df_final['icu_normalized_watts'],bins = 25)
plt.xlabel('icu_normalized_power')
plt.ylabel('Frequency')
plt.title('Distribution of icu_normalized_power')
plt.show()

# remove those out of lower bound and upper bound
def remove_outliers(dataframe, column):
    q1 = dataframe[column].quantile(0.25)
    q3 = dataframe[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Filter out rows with values outside the lower and upper bounds
    filtered_data = dataframe[(dataframe[column] >= lower_bound) & (dataframe[column] <= upper_bound)]

    return filtered_data

filtered_data = remove_outliers(df_final, 'icu_normalized_watts')

# Visualize the distribution of icu_normalized_power column after removing outliers
plt.figure(figsize=(10, 6))
plt.hist(filtered_data['icu_normalized_watts'], bins=20)
plt.xlabel('icu_normalized_watts')
plt.ylabel('Frequency')
plt.title('Distribution of icu_normalized_power (After removing outliers)')
plt.show()

filtered_data

processed_data = preprocess_data(filtered_data, "date", "icu_normalized_watts")
processed_data

prophetdata = processed_data # reset prophetdata so we could directly use the codes above.

# create prophet model with daily based data

  # m = Prophet(weekly_seasonality = True)
  m = Prophet(changepoint_prior_scale=1 , yearly_seasonality=True, weekly_seasonality = True)

  #m.add_country_holidays(country_name='US')
  m.fit(prophetdata)
  future = m.make_future_dataframe(periods=365, freq='D') # make future DF with 365 days
  future.tail()
  forecast = m.predict(future)
  fig1 = m.plot(forecast)
  fig2 = m.plot_components(forecast)
  a = add_changepoints_to_plot(fig1.gca(), m, forecast)

"""- set up capacity

We must specify the carrying capacity in a column cap. Here we will assume a particular value, but this would usually be set using data or expertise about the market size.
"""

# Python
prophetdata['cap'] = 230

# create prophet model with daily based data
  m = Prophet(yearly_seasonality = True, growth = "logistic")
  #m.add_country_holidays(country_name='US')
  m.fit(prophetdata)
  future = m.make_future_dataframe(periods=1826, freq='D') # make future DF with 365 days
  future["cap"] = 230
  future.tail()
  forecast = m.predict(future)
  fig1 = m.plot(forecast)
  fig2 = m.plot_components(forecast)
  a = add_changepoints_to_plot(fig1.gca(), m, forecast)



"""# ARIMA/ SARIMAX

## 1. ARIMA
"""

from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA

!pip install --upgrade statsmodels

# fitness = prophetdata
# fitness.index = fitness['ds']
# del fitness['ds']

fitness = df_weekly
fitness.head()
fitness

def fill_na_with_mean(df, column):
    """
    Fills missing values in the specified column with the mean of the values
    for that column in the rows before and after the current row.

    Args:
        df: A pandas dataframe.
        column: The column to fill missing values in.

    Returns:
        The modified dataframe.
    """

    # Iterate over the rows of the dataframe, and for each row, check if the value for the specified column is missing.
    for index, row in df.iterrows():
        # If the value is missing, fill it in with the mean of the values for that column in the rows before and after the current row.
        if pd.isnull(row[column]):
            df.loc[index, column] = df.loc[(index - 1 if index > 0 else 0):(index + 1 if index < len(df) - 1 else len(df) - 1)][column].mean()

    # Return the modified dataframe.
    return df

df_weekly_clean = fill_na_with_mean(df_weekly,'y')

df_weekly_clean

fitness = df_weekly_clean
fitness.index = fitness['ds']
del fitness['ds']

pd.plotting.register_matplotlib_converters()
sns.set(rc={'figure.figsize':(14, 4)})
fitness["y"].plot(linewidth=0.75)

"""Now that we have the data parsed, we're going to work towards modeling this data using a common method in statistics: ARIMA. But to do that, we need to learn about the data a little bit first.

First, we need to learn if the data is stationary. If it's not, we need to incorporate some type of integration component in our model. Put another way, if the data has a mean that changes across time, we're going to need to do something about that later in our ARIMA.
"""

def adf_test(timeseries):
    #Perform Dickey-Fuller test:
    print ('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
       dfoutput['Critical Value (%s)'%key] = value
    print (dfoutput)

adf_test(fitness['y'])

"""The p here is > .05. The null hypothesis for both tests is that the data are non-stationary. We can difference the data set. and take a look at what differencing is"""

fitness['y_diff_1'] = fitness['y'].diff(1)

fitness

pd.plotting.register_matplotlib_converters()
sns.set(rc={'figure.figsize':(14, 4)})
fitness["y_diff_1"].plot(linewidth=0.75)

"""There is no moving average, or general trend, here now"""

adf_test(fitness['y_diff_1'][1:])

"""Our p value is really low, meaning we're stationary now

Check ACF/PACF now
"""

plot_acf(fitness['y_diff_1'][1:]);

plot_pacf(fitness['y_diff_1'][1:]);

model = ARIMA(fitness['y'], order=(2,2,1))
model_fit = model.fit()
summary = model_fit.summary().as_text()
for aline in summary.split(','):
  print(aline)

"""One final check I consistently see is a Durbin Watson test. We're looking for a value closer to 2 than 1.

A Durbin Watson ranges 0-4. We're looking for a value close to 2. This suggests no autocorrelations amongst residuals, e.g., that we've modeled just about all we can out of the data with lags. That's the case here!
"""

model_fit.resid.head()

residuals = pd.DataFrame(model_fit.resid)
sns.distplot(residuals, hist=True, kde=True)

from statsmodels.stats.stattools import durbin_watson
print(durbin_watson(residuals))

model_fit.aic

"""## 2. SARIMAX"""

import statsmodels.api as sm

# Run SARIMAX without seasonality
model = sm.tsa.statespace.SARIMAX(fitness['y'], order=(2,2,1))
model_fit = model.fit(disp=False)
print(model_fit.summary())

# With Seasonality set to be quarterly.
model = sm.tsa.statespace.SARIMAX(fitness['y'], order=(2,2,1) , seasonal_order=(1,1,1,4))
model_fit = model.fit(disp=False)
print(model_fit.summary())

# With Seasonality set to be monthly.
model = sm.tsa.statespace.SARIMAX(fitness['y'], order=(2,2,1) , seasonal_order=(1,1,1,12))
model_fit = model.fit(disp=False)
print(model_fit.summary())

# With Seasonality set to be weekly.
# Use the best parameter we got from fine-tuning
# Best SARIMAX parameters: (0, 1, 0, 0, 1, 0, 52)
model = sm.tsa.statespace.SARIMAX(fitness['y'], order=(0,1,0) , seasonal_order=(0,1,0,52))
model_fit = model.fit(disp=False)
print(model_fit.summary())

model_fit.aic

residuals = pd.DataFrame(model_fit.resid)
sns.distplot(residuals, hist=True, kde=True)

print(durbin_watson(residuals))

"""[Upper Left Plot] - The residuals appear to be fairly uninform across time.

[Upper Right] - Our residuals look fairly well distributed overall.
"""

# Forecasting
pred = model_fit.get_prediction()
pred_ci = model_fit.conf_int()
ax = fitness['y'].plot(label='observed')
pred.predicted_mean.plot(ax=ax, label='forecast', alpha=.7, figsize=(14, 4))
plt.legend()
plt.show()

fitness

fcast = model_fit.predict(len(fitness['y']),len(fitness['y'])+52)

ax = fcast.plot(label='future forecast', color='red')
pred.predicted_mean.plot(ax=ax, label='forecast', alpha=.7, figsize=(14, 4))
plt.legend()
plt.show()

"""## SARIMAX fine tuning"""

# import itertools
# import numpy as np
# import pandas as pd
# import statsmodels.api as sm

# # Step 2: Prepare your time series data and split it into training and testing sets
# # Assuming your fitness data is stored in a pandas DataFrame called "fitness"
# # fitness = pd.read_csv("fitness_data.csv")
# train_data = fitness["y"]# [:-n]  # Adjust "n" to define the size of your testing set

# # Step 3: Define a function to generate all possible combinations of parameters for SARIMAX
# def generate_sarimax_parameters(p_values, d_values, q_values, P_values, D_values, Q_values, s_values):
#     parameters = itertools.product(p_values, d_values, q_values, P_values, D_values, Q_values, s_values)
#     return list(parameters)

# # Step 4: Define a function to evaluate a given set of SARIMAX parameters using a given metric
# def evaluate_sarimax(parameters, train_data, metric='aic'):
#     order = parameters[:3]
#     seasonal_order = parameters[3:]
#     try:
#         model = sm.tsa.statespace.SARIMAX(train_data, order=order, seasonal_order=seasonal_order)
#         fitted_model = model.fit(disp=False)
#         if metric == 'aic':
#             return fitted_model.aic
#         elif metric == 'bic':
#             return fitted_model.bic
#         # Add more metrics as needed
#         else:
#             return None
#     except:
#         return None

# # Step 5: Specify the range of values for each parameter that you want to tune
# p_values = [0,1,2]
# d_values = [0,1]
# q_values = [0,1,2]
# P_values = [0,1,2]
# D_values = [0, 1]
# Q_values = [0,1,2]
# s_values = [12,52]  # Seasonal period

# # Step 6: Generate all combinations of parameters
# parameters_grid = generate_sarimax_parameters(p_values, d_values, q_values, P_values, D_values, Q_values, s_values)

# # Step 7: Iterate over the parameter combinations and evaluate each model
# best_metric = float('inf')
# best_parameters = None

# for parameters in parameters_grid:
#     metric_value = evaluate_sarimax(parameters, train_data, metric='aic')
#     if metric_value is not None and metric_value < best_metric:
#         best_metric = metric_value
#         best_parameters = parameters

# # Step 8: Fit the final SARIMAX model using the best parameters on the entire training dataset
# best_order = best_parameters[:3]
# best_seasonal_order = best_parameters[3:]
# final_model = sm.tsa.statespace.SARIMAX(train_data, order=best_order, seasonal_order=best_seasonal_order)
# fitted_final_model = final_model.fit(disp=False)

# # Step 9: Use the fitted_final_model object to make predictions on the test dataset or future data
# # test_data = fitness["y"][-n:]  # Adjust "n" to match the size of your testing set
# # predictions = fitted_final_model.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)

# # Print the best parameters and predictions
# print("Best SARIMAX parameters:", best_parameters)
# # print("Predictions:", predictions)

# # Result:
# # Best SARIMAX parameters: (0, 1, 0, 0, 1, 0, 52)

"""## Fine tuning - auto arima"""

# !pip install pmdarima

# from statsmodels.tsa.arima_model import ARIMA
# from pmdarima.arima import auto_arima
# # Fit the model using auto.arima()
# model = auto_arima(fitness['y'])
# # Print the summary of the model
# print(model.summary())

"""Best parameter is (2,2,1)"""



"""# Pandas Ai playground"""

# # Pandas ai
# llm = OpenAI(api_token="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")
# pandas_ai = PandasAI(llm) # verbose=True, conversational=True; with those 2 arguments we can talk with llm

# response = pandas_ai(df_filtered, "How many types are there in the dataframe?")
# print(response)

# pandas_ai(prophetdata, "use this dataframe to train a sarimax model")